{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to ADLINK Neuron's Documents\n\n\n\n\n\n\nWhy Neuron ?\n\n\nADLINK Technology's industrial-grade ROS development kit, Neuron, boasts the characteristic in which the CPU can be change according to requirements, and has a wide variety of I/O pins. \n\n\nNeuron also could integrate GPU for supporting the AI computing platforms, which brings about great acceleration for robot computation to meet your needs for robot applications. Moreover, Neuron is compatible with ROS2, allowing to control the robot through the ROS2 application library with rich open source, such as cameras, navigation, and motion control, so that you can develop the application within the shortest possible time and reduce time to market.",
            "title": "Introduction"
        },
        {
            "location": "/#welcome-to-adlink-neurons-documents",
            "text": "",
            "title": "Welcome to ADLINK Neuron's Documents"
        },
        {
            "location": "/#why-neuron",
            "text": "ADLINK Technology's industrial-grade ROS development kit, Neuron, boasts the characteristic in which the CPU can be change according to requirements, and has a wide variety of I/O pins.   Neuron also could integrate GPU for supporting the AI computing platforms, which brings about great acceleration for robot computation to meet your needs for robot applications. Moreover, Neuron is compatible with ROS2, allowing to control the robot through the ROS2 application library with rich open source, such as cameras, navigation, and motion control, so that you can develop the application within the shortest possible time and reduce time to market.",
            "title": "Why Neuron ?"
        },
        {
            "location": "/sema-user-guide/",
            "text": "Neuron SEMA User Guide",
            "title": "Neuron SEMA User Guide"
        },
        {
            "location": "/sema-user-guide/#neuron-sema-user-guide",
            "text": "",
            "title": "Neuron SEMA User Guide"
        },
        {
            "location": "/neuron-ddsbot/",
            "text": "Neuron DDSBot",
            "title": "Neuron DDSBot"
        },
        {
            "location": "/neuron-ddsbot/#neuron-ddsbot",
            "text": "",
            "title": "Neuron DDSBot"
        },
        {
            "location": "/neuron-neuronbot/",
            "text": "Neuron NeuronBot",
            "title": "Neuron NeuronBot"
        },
        {
            "location": "/neuron-neuronbot/#neuron-neuronbot",
            "text": "",
            "title": "Neuron NeuronBot"
        },
        {
            "location": "/neuron-omnibot/",
            "text": "Neuron Omni Bot\n\n\nThis is a demo package of Neuron Omni Bot's autonomous and fast integration capability using ROS\n\n\n\n\nFeatures\n\n\n\n\nFully integration between SEMA library and ROS2\n\n\nThree omni-directional poly wheels with a 12V 4.32 watt DC motor on each wheel\n\n\nQuadrature hall effect encoder provide 390 pulses (1560 count) per wheel revolution\n\n\nFully customized \nVDMC\n using STM32F103 chip\n\n\nFully customized ROS motor driver node that utilize standard ROS convension\n\n\nMPU6050 6-axis IMU and it's corresponding ROS publisher with standard ROS state estimator\n\n\nEasy to use ROS launch file with maximum custimization capabilities\n\n\nFinely tuned ROS navigation, guidance, and control algorithms with full list of parameters provided\n\n\nMultiple user interfaing methods using the SEMA library\n\n\n\n\nGetting Started\n\n\nThese instructions will get you a copy of this demo and running on your local machine. If you're unfamiliar with ROS operation, there are some usefull trick you may want to try in the \nsection below\n For multiple machine remote control, please visit \nmulti-machines\n section below.\n\n\n\nPrerequisites\n\n\n\n\nNeuron hardware and SEMA library\n\nYou'll need the ADLINK SEMA library and a compatible motherboard to run this example. You can conatct \nTODO: SOMEONE\n for more information.\n\n\nDownload the source of this project to your ROS (catkin) workspace\n\n\ncd catkin_ws/src\n    git clone https://github.com/EwingKang/Neuron-OmniBot.git neuron_omnibot\n  \n\n\nSome electronics hardware: 3 LEDs, 3 contact switches, 1 tactile(push) switch\n\n\nLaser scanner with its corresponding ROS node properly installed.\n\n\nWires properly connected (see \nhardware setup section\n below)\n\n\n\n\nInstalling\n\n\na) hardware setup\n\n\nConnect all the wire properly according to this diagram: (\nclick to download\n)\n\n\n\n\n\nVDMC\n \nmanual\n\n\nconnect 12V power from Neuron PSU\n\n\nConnect UART port to MAX232 or any equivilant UART-RS232 bridge chip (\nNOTE: some manufacture may have their RX/TX label reversed\n)\n\n\nconnect to Neuron serial port 2\n\n\n\n\n\n\nLaser scanner (ex. YDLidar)\n\n\nUSB wire\n\n\n5v power (from SEMA feature connector SB5V is recommand)\n\n\n\n\n\n\nSEMA peripherals (collision detection, state indication LEDs)\n\n\nLEDs: positive to GPIO, Negative to GND\n\n\nswitches: across GPIO and GND\n\n\n\n\n\n\nOther recommandations\n\n\nIt is recommanded to have your robot's sharp edges wraped\n\n\nDO NOT obstruct the view of laser scanner\n\n\nYour two wifi antennas should be pointing perpendicular(i.e. 90 degrees) to each other\n\n\nALWAYS put on your balance lead monitor if your using Li-Po batteries\n\n\n\n\n\n\n\n\nb) software setup\n\n\n\n\n\n\nInstall ADLINK SEMA\nYour Neuron Bot should already have proper SEMA installed. Please go to \nTODO: EMPTY PROJECT\n if you have any questions.\n\n\n\n\n\n\nInstall ROS kinetic and setup workspace\n    Your Neuron Bot should already have ROS set. If not, you mar refer to \nthe install guide\n, and \ncatkin_ws setup guide\n. Make sure you have environmental path add to .bashrc to save time\n\n\n\n\n\n\nInstall packages (ubuntu software):\n\n\n\n\n\n\nROS stuff\n\n\n\n\n\n\nrobot localization\n\nsudo apt-get install ros-kinetic-robot-localization\n\n\n\n\n\n\nlaser slam\n\nsudo apt-get install ros-kinetic-gmapping ros-kinetic-scan-tools\\ ros-kinetic-navigation # laser slam\n\n\n\n\n\n\nnavigation and planning\n\nsudo apt-get install ros-kinetic-teb-local-planner ros-kinetic-teb-local-planner-tutorials\\ ros-kinetic-eband-local-planner\n\n\n\n\n\n\n\n\n\n\nRecommanded\n\n\n\n\nKATE: text editor (very similar to Notepad++)\n\nsudo apt-get install kate\n\n\nhtop: a low-cost system monitor\n\nsudo apt-get install htop\n\n\nserial port terminoa (GUI monitor) \n\nsudo apt-get install gtkterm\n\n\nOBS\n\n\nSSH server\n\nsudo apt-get install openssh-server\n\n\n\n\n\n\n\n\n\n\n\n\nSet up SEMA soft link\nChange to any node that uses SEMA library, find the SEMA include library header location, and run the auto shell command file. \n    \ncd ${project_root}/lib\n        e.g.: cd catkin_ws/src/neuron_omnibot/neuron_demo_gpio/lib\n    ./setlink.sh\n\n    Note: If you get some error like \nerror: no such file as...\n, you'll need to make the setlink.sh executable by \nchmod +x setlink.sh\n after you've changed the command prompt to that directory.      \n\n\n\n\n\n\nCompile the source code\n\nNow, we'll use the Catkin, the ROS build management tool to build our nodes. We'll need root access for library linking for anything that uses SEMA. Root access is gained by the second step below. Great power comes with great responsibility, \nit is strongly recommanded you to exit root mode\n since you can to terrible stuff with that much of power.\n    \ncd ~/catkin_ws\n    sudo -sE\n    catkin_make\n    exit   #exit root mode\n  \n\n\n\n\n\n\nSetup Laser scanner port (from \nYDLidar github\n)\n    \nroscd ydlidar/startup\n    sudo chmod 777 ./*\n    sudo sh initenv.sh\n\n\n\n\n\n\nAdd serial access\n    \nsudo adduser ros dialout\n\n\n\n\n\n\nRun the demo\n\n\nThe Neuron Omnibot demo can be divided into four part:\n\n \nOmnibot IO\n: motor controller, laser scanner, LED indecators, and servos\n\n \nSLAM\n: simultaneous localization and mapping\n\n \nLocalization\n: after we build our 2D map\n\n \nMove!\n: Obstacle detecting, planning, trajectory generation, and vehicle control  \n\n\nEach of the above function is wraped as a single ROS launch file for user's easy execution. We'll have a step-by-step tutorial below. For each launch file, we'll open a new terminal. You can do that by pressing \nctrl + alt + t\n  \n\n\na) OmniBot driver\n\n\nIn this section, we'll start our ROS omnibot driver. The driver includes all the IO and sensory device including motor controller, encoder odometry, laser scanner, and IMU state estimation.\n\n\n\n\n\n\n\n\n\n\n\n\nroscore\n\nroscore is the core of the ROS as its name suggest. We encourage you to manually start the core on a seperate window because it gives user the power and responsibility to control everything.\n    \nroscore\n\n\n\n\n\n\nRobot Base driver\n    This launch file include multiple node. It launches the communication between STM32 motor controller, laser slam, as well as all the robot TF definition. Please note that if you're ending the node by \nctrl + c\n, you only have to hit once and give it a seconds for it to shutdown automatically. The LaserScan node requies some time to shutdown the serial port.\n    \nroslaunch omni_base_driver omni_localization.launch\n\n\n\n\nKeyboard controller node\nWe use \nteleop_twist_keyboard\n as our manual driver. The default command is a little too fast, so use \nx\n and \nc\n to reduce velocity to around 0.3. \n    \nrosrun teleop_twist_keyboard teleop_twist_keyboard.py\n\n    Note: because Neuron OmniBot is a holonomic robot, you can hold \nshift\n+moving around command to do translational moving.\n\n\nRVIZ monitoring\n    RVIZ stands for ROS-VIsualiZation, which is a powerfull 3-D visualization environment. We can launch RVIZ simply by:\n    \nrviz\n\n    Now, use the \"File -> Open Config\" or \nctrl + o\n to  open base  visualization settings located at \n$(omni_base_driver)/rviz_config/omni_driver_laser.rviz\n. One should note that despite it is easier to simply use the gui provided to open the rviz config file, it is possible to use command line by adding \nabsolute path\n:\n    \nrviz -d \"/home/ros/catkin_ws/src/neuron_omnibot/omni_base_slam/rviz_config/omni_slam.rviz\"\n\n\n\n\nb) Laser Slam\n\n\nIn this section, we'll build our map with our 2D laser scanner.\n\n\n\n\n\n\n\n\n\n\nMake sure you have everything in the \nbase driver\n  launched. This includes all the robot TF, motor driver, and laser scanner. \n\n\nSetup rviz correctly so we can see everything:\n   you can open \n($ omni_base_slam)/rviz_config/omni_slam.rviz\n manually, or by running the following command:\n   \nrviz -d \"/home/ros/catkin_ws/src/neuron_omnibot/omni_base_slam/rviz_config/omni_slam.rviz\"\n\n\nLet's start the laser localization and mapping procedure with \ngmapping\n by the following command:\n    \nroslaunch omni_base_slam omni_gmapping.launch\n\n\nDrive around using keyboard driver introduced in \nbase driver\n.\n\n\nAfter you map the whole place, remember to save the map \nbefore\n closing the gmapping:\n    \nrosrun map_server map_saver -f map_file_name\n\n    A map file(.x)  and a config file (.xxx) will be saved under your user home \n~/\n, make sure to move both of these files to \n($ omni_base_slam)/map/\n\n\nStop the gmapping by \nctrl + c\n on the gmapping terminal (terminal of the step.4).\n\n\n\n\nc) Robot Localization\n\n\nAfter we acquired a static map, it is often that we do not run a SLAM package all the time due to its comutational load. In this step, we'll uses the the AMCL package to help us find the robot's location given previously generated map and current laser scan.\n\n\n\n\n\n\n\n\n\nMake sure you have everything in the \nbase driver\n  launched. This includes all the robot TF, motor driver, and laser scanner. \n\n\nSetup rviz correctly so we can see everything:\n   you can open \n($ omni_base_nav)/rviz_config/omni_amcl.rviz\n with rviz gui, or by running the following command:\n       \nrviz -d \"/home/ros/catkin_ws/src/neuron_omnibot/omni_base_nav/rviz_config/omni_amcl.rviz\"\n\n\nPut the map file and its config file to \n($ omni_base_slam)/map/\n as stated in the previous section. Modify line 4 of \n($ omni_base_nav)/omni_localize.launch\n,, the \nargs=\n tags in the map server, to reflect the correct file name.\n\n\nNow, we'll start our localization package \namcl\n with our costumized settings:\n    \nroslaunch omni_base_nav omni_localize.launch\n\n    By default, the localization package will initialize the robot at (x,y)=(0,0), i.e. same as the starting pose when we started the mapping process. However, we can manually assign the starting position by using \"set 2D pose estimation\" function in the RVIZ if it's not the case. Select the tool, click on the position and drag the arrow for its initial heading. This is shown in the picture below, the \"2D pose estimation\" is marked by a red square at the upper banner.\n    \n\n\nNow we've initialize the robot pose, we can see many arrows in the RVIZ world. These arrows are the particles used to localize the robot. Because of how the Monte Carlo method (AMCL) works, those poses will not update nor converge if the robot remains still. With that being said, we can still ask the localization to try to update by:\n    \nrosservice call /request_nomotion_update\n\n    And you'll see the arrows become more unison and your laser scaning pattern will gradually match with the map.\n\n\nIt is often possible for robot to identify it's initial location without manually set the initiali pose. You can call this service so the localization package will evenly distribute the pose particle. After the global initialization, you can perform multiple no-motion-update mentioned above  and the package will localize itself.\n    \nrosservice call /global_localization\n\n\n\n\nd) Navigation\n\n\nWith OmniBot localized, we can now do our planning. In this demonstratino, we utilize the \nmove base\n structure in the ROS \nnavigation stack\n. This is a very textbook and complete structure of such system.\n\n\n\n\n\n\n\n\n\nbase driver\n is started\n\n\nlocalization\n is initilized\n\n\nRVIZ is set to \n($ omni_base_nav)/rviz_config/omni_nav.rviz\n\n\nThere are three different popular local planning package for you to choose: the \nDynamic Window Approach\n, the \nTimed Elastic Band\n planner, and the \nElastic Band\n planner. You can choose from one of the following command:\n    \nroslaunch omni_base_nav omni_nav_dwa.launch\n    roslaunch omni_base_nav omni_nav_teb.launch\n    roslaunch omni_base_nav omni_nav_eband.launch\n\n\nAfter the planner is started, you should see something like the graph below in the RVIZ. You can choose the \"2D nav goal\" tools on the top banner of the RVIZ. Click on the map and drage to specify the target orientation. The robot should drive toward the goal by itself.\n\n\nThe background gray map is the global map, while the colorful blue-pink-red one is the 1.5x1.5 meter local map. The global path is drawn in cyan while the local planner is draw in blue. Different local planner will have different representation.\n\n\n\n\nMultiple Machines\n\n\nTypically, it is not so convenient to have your mouse, keyboard and monitor connect to the robot while the robot move around. Fortunately, ROS is constructed in such a way that running the same ROS across multiple machines is very easy. In fact, there is a \nsimple official tutorial\n about it.\n\nThe physical Networking can be easily achieved (and expanded) with a single wifi router as shown below:\n\n\n\n\n\nFirst, findout the OmniBot's IP address. This can be done with linux command \nifconfg\n.\n\n\nsetup environmental variable. We'll add the variable to the hidden file \"\n.bashrc\n\". You can \nctrl+h\n to show hidden file, and edit the \"\n.bashrc\n\" manually or by following command:\n    \necho 'export ROS_IP=YOUR_OMNIBOT_IP' >> ~/.bashrc\n    echo 'export ROS_MASTER_URI=http://$ROS_IP:11311' >> ~/.bashrc\n\n    This step should be done on both station and the OmniBot.  \n\n\nSince we don't have monitor nor keyboard connected, everything will be executed remotely. First, Let's make sure our machines can reach each other by\n    \nping NeuronBot.local\n    # or\n    ping YOUR_OMNIBOT_IP\n\n\n\n\nWe'll setup the SSH connection. (In case you don't have that on the OmniBot, do that now as this \nprevious section\n has said). This can be easily done by:\n    \nssh NeuronBot.local\n    # or\n    ssh YOUR_OMNIBOT_IP\n\n\n\n\n\n\nAt the OmniBot SSH connection, start byobu so we can have multiple terminal in a single SSH connection\n    \nbyobu\n\n    now, you can add new terminal with \nF2\n, switch between terminals with \nF3\n and \nF4\n, exit with command \nexit\n\n\n\n\nRun omniBot nodes on the Omni Bot through SSH terminal. \n\n\nRVIZ and all the other things on the station machine\n\n\nPlay and fun!\n\n\n\n\nUseful tricks\n\n\nLinux/Ubuntu terminal\n\n\n\n\nctrl + shift + T\n to open new terminal tab\n\n\nctrl + PageUp/PageDn\n to switch between tabs\n\n\n\n\nROS tools\n\n\n\n\nrqt\n ROS - QT, ROS information visualization tools\n\n\nrostopic\n: ROS topic server functions\n\n\nrostopic list\n  to list all topics \n\n\nrostopic echo /SOME_TOPIC\n to print the topic directly.\n\n\n\n\n\n\nrosservice\n  : ROS service server functions\n\n\n\n\nMore Info\n\n\nVideos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReference\n\n\nNeuron VDMC Communication and operation manual\n\n\nNotice\n\n\n\n\nSEMA requries root access since it needs to talk to the hardware. Execute \nSUDO -sE\n before program launch to retain proper shell environment variable.\n\n\n\n\nVersion\n\n\n0.3.0\n\n\nAuthors\n\n\n\n\nEwing Kang\n - \nVDMC algorithm developer/ ROS implementation\n - (https://github.com/EwingKang)\n\n\nAlan Chen\n - \nSEMA library example\n - (alan.chen@adlinktech.com)\n\n\n\n\nLicense\n\n\nThis project is licensed under the Apache License, Version 2.0\n\n\nAcknowledgments\n\n\nThe development of this project is under support and collabration of ADLINK Advanced Robotic Platform Group(ARPG). \n\n\nfuture roadmap\n\n\n\n\n[ ] Object recgonition and tracking using movidius\n\n\n[ ] Peripheral sensors and robot casing (with fully SEMA integration)\n\n\n[ ] VDMC \n\n\n[ ] On-board Kalman filter with full state omniBot dynamics estimater\n\n\n[ ] High performance nonlinear-dynamic inversion controller",
            "title": "Neuron OmniBot"
        },
        {
            "location": "/neuron-omnibot/#neuron-omni-bot",
            "text": "This is a demo package of Neuron Omni Bot's autonomous and fast integration capability using ROS",
            "title": "Neuron Omni Bot"
        },
        {
            "location": "/neuron-omnibot/#features",
            "text": "Fully integration between SEMA library and ROS2  Three omni-directional poly wheels with a 12V 4.32 watt DC motor on each wheel  Quadrature hall effect encoder provide 390 pulses (1560 count) per wheel revolution  Fully customized  VDMC  using STM32F103 chip  Fully customized ROS motor driver node that utilize standard ROS convension  MPU6050 6-axis IMU and it's corresponding ROS publisher with standard ROS state estimator  Easy to use ROS launch file with maximum custimization capabilities  Finely tuned ROS navigation, guidance, and control algorithms with full list of parameters provided  Multiple user interfaing methods using the SEMA library",
            "title": "Features"
        },
        {
            "location": "/neuron-omnibot/#getting-started",
            "text": "These instructions will get you a copy of this demo and running on your local machine. If you're unfamiliar with ROS operation, there are some usefull trick you may want to try in the  section below  For multiple machine remote control, please visit  multi-machines  section below.",
            "title": "Getting Started"
        },
        {
            "location": "/neuron-omnibot/#prerequisites",
            "text": "Neuron hardware and SEMA library \nYou'll need the ADLINK SEMA library and a compatible motherboard to run this example. You can conatct  TODO: SOMEONE  for more information.  Download the source of this project to your ROS (catkin) workspace  cd catkin_ws/src\n    git clone https://github.com/EwingKang/Neuron-OmniBot.git neuron_omnibot     Some electronics hardware: 3 LEDs, 3 contact switches, 1 tactile(push) switch  Laser scanner with its corresponding ROS node properly installed.  Wires properly connected (see  hardware setup section  below)",
            "title": "Prerequisites"
        },
        {
            "location": "/neuron-omnibot/#installing",
            "text": "",
            "title": "Installing"
        },
        {
            "location": "/neuron-omnibot/#a-hardware-setup",
            "text": "Connect all the wire properly according to this diagram: ( click to download )   VDMC   manual  connect 12V power from Neuron PSU  Connect UART port to MAX232 or any equivilant UART-RS232 bridge chip ( NOTE: some manufacture may have their RX/TX label reversed )  connect to Neuron serial port 2    Laser scanner (ex. YDLidar)  USB wire  5v power (from SEMA feature connector SB5V is recommand)    SEMA peripherals (collision detection, state indication LEDs)  LEDs: positive to GPIO, Negative to GND  switches: across GPIO and GND    Other recommandations  It is recommanded to have your robot's sharp edges wraped  DO NOT obstruct the view of laser scanner  Your two wifi antennas should be pointing perpendicular(i.e. 90 degrees) to each other  ALWAYS put on your balance lead monitor if your using Li-Po batteries",
            "title": "a) hardware setup"
        },
        {
            "location": "/neuron-omnibot/#b-software-setup",
            "text": "Install ADLINK SEMA\nYour Neuron Bot should already have proper SEMA installed. Please go to  TODO: EMPTY PROJECT  if you have any questions.    Install ROS kinetic and setup workspace\n    Your Neuron Bot should already have ROS set. If not, you mar refer to  the install guide , and  catkin_ws setup guide . Make sure you have environmental path add to .bashrc to save time    Install packages (ubuntu software):    ROS stuff    robot localization sudo apt-get install ros-kinetic-robot-localization    laser slam sudo apt-get install ros-kinetic-gmapping ros-kinetic-scan-tools\\ ros-kinetic-navigation # laser slam    navigation and planning sudo apt-get install ros-kinetic-teb-local-planner ros-kinetic-teb-local-planner-tutorials\\ ros-kinetic-eband-local-planner      Recommanded   KATE: text editor (very similar to Notepad++) sudo apt-get install kate  htop: a low-cost system monitor sudo apt-get install htop  serial port terminoa (GUI monitor)  sudo apt-get install gtkterm  OBS  SSH server sudo apt-get install openssh-server       Set up SEMA soft link\nChange to any node that uses SEMA library, find the SEMA include library header location, and run the auto shell command file. \n     cd ${project_root}/lib\n        e.g.: cd catkin_ws/src/neuron_omnibot/neuron_demo_gpio/lib\n    ./setlink.sh \n    Note: If you get some error like  error: no such file as... , you'll need to make the setlink.sh executable by  chmod +x setlink.sh  after you've changed the command prompt to that directory.          Compile the source code \nNow, we'll use the Catkin, the ROS build management tool to build our nodes. We'll need root access for library linking for anything that uses SEMA. Root access is gained by the second step below. Great power comes with great responsibility,  it is strongly recommanded you to exit root mode  since you can to terrible stuff with that much of power.\n     cd ~/catkin_ws\n    sudo -sE\n    catkin_make\n    exit   #exit root mode       Setup Laser scanner port (from  YDLidar github )\n     roscd ydlidar/startup\n    sudo chmod 777 ./*\n    sudo sh initenv.sh    Add serial access\n     sudo adduser ros dialout",
            "title": "b) software setup"
        },
        {
            "location": "/neuron-omnibot/#run-the-demo",
            "text": "The Neuron Omnibot demo can be divided into four part:   Omnibot IO : motor controller, laser scanner, LED indecators, and servos   SLAM : simultaneous localization and mapping   Localization : after we build our 2D map   Move! : Obstacle detecting, planning, trajectory generation, and vehicle control    Each of the above function is wraped as a single ROS launch file for user's easy execution. We'll have a step-by-step tutorial below. For each launch file, we'll open a new terminal. You can do that by pressing  ctrl + alt + t",
            "title": "Run the demo"
        },
        {
            "location": "/neuron-omnibot/#a-omnibot-driver",
            "text": "In this section, we'll start our ROS omnibot driver. The driver includes all the IO and sensory device including motor controller, encoder odometry, laser scanner, and IMU state estimation.       roscore \nroscore is the core of the ROS as its name suggest. We encourage you to manually start the core on a seperate window because it gives user the power and responsibility to control everything.\n     roscore    Robot Base driver\n    This launch file include multiple node. It launches the communication between STM32 motor controller, laser slam, as well as all the robot TF definition. Please note that if you're ending the node by  ctrl + c , you only have to hit once and give it a seconds for it to shutdown automatically. The LaserScan node requies some time to shutdown the serial port.\n     roslaunch omni_base_driver omni_localization.launch   Keyboard controller node\nWe use  teleop_twist_keyboard  as our manual driver. The default command is a little too fast, so use  x  and  c  to reduce velocity to around 0.3. \n     rosrun teleop_twist_keyboard teleop_twist_keyboard.py \n    Note: because Neuron OmniBot is a holonomic robot, you can hold  shift +moving around command to do translational moving.  RVIZ monitoring\n    RVIZ stands for ROS-VIsualiZation, which is a powerfull 3-D visualization environment. We can launch RVIZ simply by:\n     rviz \n    Now, use the \"File -> Open Config\" or  ctrl + o  to  open base  visualization settings located at  $(omni_base_driver)/rviz_config/omni_driver_laser.rviz . One should note that despite it is easier to simply use the gui provided to open the rviz config file, it is possible to use command line by adding  absolute path :\n     rviz -d \"/home/ros/catkin_ws/src/neuron_omnibot/omni_base_slam/rviz_config/omni_slam.rviz\"",
            "title": "a) OmniBot driver"
        },
        {
            "location": "/neuron-omnibot/#b-laser-slam",
            "text": "In this section, we'll build our map with our 2D laser scanner.      Make sure you have everything in the  base driver   launched. This includes all the robot TF, motor driver, and laser scanner.   Setup rviz correctly so we can see everything:\n   you can open  ($ omni_base_slam)/rviz_config/omni_slam.rviz  manually, or by running the following command:\n    rviz -d \"/home/ros/catkin_ws/src/neuron_omnibot/omni_base_slam/rviz_config/omni_slam.rviz\"  Let's start the laser localization and mapping procedure with  gmapping  by the following command:\n     roslaunch omni_base_slam omni_gmapping.launch  Drive around using keyboard driver introduced in  base driver .  After you map the whole place, remember to save the map  before  closing the gmapping:\n     rosrun map_server map_saver -f map_file_name \n    A map file(.x)  and a config file (.xxx) will be saved under your user home  ~/ , make sure to move both of these files to  ($ omni_base_slam)/map/  Stop the gmapping by  ctrl + c  on the gmapping terminal (terminal of the step.4).",
            "title": "b) Laser Slam"
        },
        {
            "location": "/neuron-omnibot/#c-robot-localization",
            "text": "After we acquired a static map, it is often that we do not run a SLAM package all the time due to its comutational load. In this step, we'll uses the the AMCL package to help us find the robot's location given previously generated map and current laser scan.     Make sure you have everything in the  base driver   launched. This includes all the robot TF, motor driver, and laser scanner.   Setup rviz correctly so we can see everything:\n   you can open  ($ omni_base_nav)/rviz_config/omni_amcl.rviz  with rviz gui, or by running the following command:\n        rviz -d \"/home/ros/catkin_ws/src/neuron_omnibot/omni_base_nav/rviz_config/omni_amcl.rviz\"  Put the map file and its config file to  ($ omni_base_slam)/map/  as stated in the previous section. Modify line 4 of  ($ omni_base_nav)/omni_localize.launch ,, the  args=  tags in the map server, to reflect the correct file name.  Now, we'll start our localization package  amcl  with our costumized settings:\n     roslaunch omni_base_nav omni_localize.launch \n    By default, the localization package will initialize the robot at (x,y)=(0,0), i.e. same as the starting pose when we started the mapping process. However, we can manually assign the starting position by using \"set 2D pose estimation\" function in the RVIZ if it's not the case. Select the tool, click on the position and drag the arrow for its initial heading. This is shown in the picture below, the \"2D pose estimation\" is marked by a red square at the upper banner.\n      Now we've initialize the robot pose, we can see many arrows in the RVIZ world. These arrows are the particles used to localize the robot. Because of how the Monte Carlo method (AMCL) works, those poses will not update nor converge if the robot remains still. With that being said, we can still ask the localization to try to update by:\n     rosservice call /request_nomotion_update \n    And you'll see the arrows become more unison and your laser scaning pattern will gradually match with the map.  It is often possible for robot to identify it's initial location without manually set the initiali pose. You can call this service so the localization package will evenly distribute the pose particle. After the global initialization, you can perform multiple no-motion-update mentioned above  and the package will localize itself.\n     rosservice call /global_localization",
            "title": "c) Robot Localization"
        },
        {
            "location": "/neuron-omnibot/#d-navigation",
            "text": "With OmniBot localized, we can now do our planning. In this demonstratino, we utilize the  move base  structure in the ROS  navigation stack . This is a very textbook and complete structure of such system.     base driver  is started  localization  is initilized  RVIZ is set to  ($ omni_base_nav)/rviz_config/omni_nav.rviz  There are three different popular local planning package for you to choose: the  Dynamic Window Approach , the  Timed Elastic Band  planner, and the  Elastic Band  planner. You can choose from one of the following command:\n     roslaunch omni_base_nav omni_nav_dwa.launch\n    roslaunch omni_base_nav omni_nav_teb.launch\n    roslaunch omni_base_nav omni_nav_eband.launch  After the planner is started, you should see something like the graph below in the RVIZ. You can choose the \"2D nav goal\" tools on the top banner of the RVIZ. Click on the map and drage to specify the target orientation. The robot should drive toward the goal by itself. \nThe background gray map is the global map, while the colorful blue-pink-red one is the 1.5x1.5 meter local map. The global path is drawn in cyan while the local planner is draw in blue. Different local planner will have different representation.",
            "title": "d) Navigation"
        },
        {
            "location": "/neuron-omnibot/#multiple-machines",
            "text": "Typically, it is not so convenient to have your mouse, keyboard and monitor connect to the robot while the robot move around. Fortunately, ROS is constructed in such a way that running the same ROS across multiple machines is very easy. In fact, there is a  simple official tutorial  about it. \nThe physical Networking can be easily achieved (and expanded) with a single wifi router as shown below:   First, findout the OmniBot's IP address. This can be done with linux command  ifconfg .  setup environmental variable. We'll add the variable to the hidden file \" .bashrc \". You can  ctrl+h  to show hidden file, and edit the \" .bashrc \" manually or by following command:\n     echo 'export ROS_IP=YOUR_OMNIBOT_IP' >> ~/.bashrc\n    echo 'export ROS_MASTER_URI=http://$ROS_IP:11311' >> ~/.bashrc \n    This step should be done on both station and the OmniBot.    Since we don't have monitor nor keyboard connected, everything will be executed remotely. First, Let's make sure our machines can reach each other by\n     ping NeuronBot.local\n    # or\n    ping YOUR_OMNIBOT_IP   We'll setup the SSH connection. (In case you don't have that on the OmniBot, do that now as this  previous section  has said). This can be easily done by:\n     ssh NeuronBot.local\n    # or\n    ssh YOUR_OMNIBOT_IP    At the OmniBot SSH connection, start byobu so we can have multiple terminal in a single SSH connection\n     byobu \n    now, you can add new terminal with  F2 , switch between terminals with  F3  and  F4 , exit with command  exit   Run omniBot nodes on the Omni Bot through SSH terminal.   RVIZ and all the other things on the station machine  Play and fun!",
            "title": "Multiple Machines"
        },
        {
            "location": "/neuron-omnibot/#useful-tricks",
            "text": "",
            "title": "Useful tricks"
        },
        {
            "location": "/neuron-omnibot/#linuxubuntu-terminal",
            "text": "ctrl + shift + T  to open new terminal tab  ctrl + PageUp/PageDn  to switch between tabs",
            "title": "Linux/Ubuntu terminal"
        },
        {
            "location": "/neuron-omnibot/#ros-tools",
            "text": "rqt  ROS - QT, ROS information visualization tools  rostopic : ROS topic server functions  rostopic list   to list all topics   rostopic echo /SOME_TOPIC  to print the topic directly.    rosservice   : ROS service server functions",
            "title": "ROS tools"
        },
        {
            "location": "/neuron-omnibot/#more-info",
            "text": "",
            "title": "More Info"
        },
        {
            "location": "/neuron-omnibot/#videos",
            "text": "",
            "title": "Videos"
        },
        {
            "location": "/neuron-omnibot/#reference",
            "text": "Neuron VDMC Communication and operation manual",
            "title": "Reference"
        },
        {
            "location": "/neuron-omnibot/#notice",
            "text": "SEMA requries root access since it needs to talk to the hardware. Execute  SUDO -sE  before program launch to retain proper shell environment variable.",
            "title": "Notice"
        },
        {
            "location": "/neuron-omnibot/#version",
            "text": "0.3.0",
            "title": "Version"
        },
        {
            "location": "/neuron-omnibot/#authors",
            "text": "Ewing Kang  -  VDMC algorithm developer/ ROS implementation  - (https://github.com/EwingKang)  Alan Chen  -  SEMA library example  - (alan.chen@adlinktech.com)",
            "title": "Authors"
        },
        {
            "location": "/neuron-omnibot/#license",
            "text": "This project is licensed under the Apache License, Version 2.0",
            "title": "License"
        },
        {
            "location": "/neuron-omnibot/#acknowledgments",
            "text": "The development of this project is under support and collabration of ADLINK Advanced Robotic Platform Group(ARPG).",
            "title": "Acknowledgments"
        },
        {
            "location": "/neuron-omnibot/#future-roadmap",
            "text": "[ ] Object recgonition and tracking using movidius  [ ] Peripheral sensors and robot casing (with fully SEMA integration)  [ ] VDMC   [ ] On-board Kalman filter with full state omniBot dynamics estimater  [ ] High performance nonlinear-dynamic inversion controller",
            "title": "future roadmap"
        }
    ]
}